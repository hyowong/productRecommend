{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf_8 -*-\n",
    "import os\n",
    "import argparse\n",
    "import cv2\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.engine.input_layer import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.layers import BatchNormalization, Lambda, AveragePooling2D\n",
    "from keras.layers import GlobalAveragePooling2D, Activation, concatenate\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.applications.densenet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "\n",
    "from model import *\n",
    "from custom_loss import *\n",
    "\n",
    "def preprocess(img):\n",
    "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return preprocess_input(img)\n",
    "\n",
    "\n",
    "def get_feature(model, DB_path):\n",
    "\n",
    "    img_size = (224, 224)\n",
    "\n",
    "    intermediate_model = Model(\n",
    "        inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "    test_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess, dtype='float32')\n",
    "\n",
    "    db_generator = test_datagen.flow_from_directory(\n",
    "        directory=DB_path,\n",
    "        classes=[\"db\"],\n",
    "        target_size=(224, 224),\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "    db_vecs = intermediate_model.predict_generator(db_generator,\n",
    "                                                   steps=len(db_generator),\n",
    "                                                   verbose=1)\n",
    "\n",
    "    return db_vecs\n",
    "\n",
    "class Descriptor():\n",
    "    def __init__(self, config):\n",
    "\n",
    "        self.input_shape = config.input_shape\n",
    "        self.sbow_shape = config.sbow_shape\n",
    "        self.num_classes = config.num_classes\n",
    "        self.batch_size = config.batch_size\n",
    "        self.nb_epoch = config.epoch\n",
    "\n",
    "        self.model = base_model(self.input_shape, self.num_classes)\n",
    "\n",
    "    def train(self, dataset_path, datagen, checkpoint_path, checkpoint_inteval):\n",
    "\n",
    "        opt = keras.optimizers.Adam(amsgrad=True)\n",
    "        model = self.model\n",
    "        model.compile(loss=ArcFaceloss, optimizer=opt)\n",
    "\n",
    "        train_generator = datagen.flow_from_directory(\n",
    "            directory=dataset_path,\n",
    "            target_size=self.input_shape[:2],\n",
    "            color_mode=\"rgb\",\n",
    "            batch_size=self.batch_size,\n",
    "            class_mode=\"categorical\",\n",
    "            shuffle=True,\n",
    "            subset='training')\n",
    "\n",
    "        val_generator = datagen.flow_from_directory(\n",
    "            directory=dataset_path,\n",
    "            target_size=self.input_shape[:2],\n",
    "            color_mode=\"rgb\",\n",
    "            batch_size=self.batch_size,\n",
    "            class_mode=\"categorical\",\n",
    "            shuffle=True,\n",
    "            subset='validation')\n",
    "        \n",
    "        \"\"\" Callback \"\"\"\n",
    "        monitor = 'loss'\n",
    "        reduce_lr = ReduceLROnPlateau(monitor=monitor, patience=4)\n",
    "\n",
    "        \"\"\" Training loop \"\"\"\n",
    "        STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size\n",
    "        STEP_SIZE_VAL = val_generator.n // val_generator.batch_size\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        for epoch in range(self.nb_epoch):\n",
    "            t1 = time.time()\n",
    "            res = model.fit_generator(generator=train_generator,\n",
    "                                      steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                                      initial_epoch=epoch,\n",
    "                                      validation_data=val_generator,\n",
    "                                      validation_steps=STEP_SIZE_VAL,\n",
    "                                      epochs=epoch + 1,\n",
    "                                      callbacks=[reduce_lr],\n",
    "                                      verbose=1,\n",
    "                                      shuffle=True)\n",
    "            t2 = time.time()\n",
    "            print(res.history)\n",
    "            print('Training time for one epoch : %.1f' % ((t2 - t1)))\n",
    "\n",
    "            if epoch % checkpoint_inteval == 0:\n",
    "                model.save_weights(os.path.join(checkpoint_path, str(epoch)))\n",
    "                \n",
    "                \n",
    "        model.save_weights(os.path.join(checkpoint_path, \"finish.hdf5\"))\n",
    "        print('Total training time : %.1f' % (time.time() - t0))\n",
    "\n",
    "    def updateDB(self, model_path, DB_path, reference_path):\n",
    "        \n",
    "        db = [file for file in os.listdir(DB_path + \"db\") if file.endswith(\".png\")]\n",
    "        print(\"db file:\", len(db))\n",
    "\n",
    "        self.model.load_weights(model_path)\n",
    "\n",
    "        features = get_feature(self.model, DB_path)\n",
    "        \n",
    "        print(\"feature's shape\", features.shape)\n",
    "        \n",
    "        reference = {}\n",
    "        \n",
    "        reference[\"img\"] = db\n",
    "        reference[\"feature\"] = list(features)\n",
    "        \n",
    "        with open(os.path.join(reference_path, \"reference.p\"), \"wb\") as f:\n",
    "            pickle.dump(reference, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print(\"UPDATE COMPLETE\")\n",
    "\n",
    "        return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters\n",
    "    args.add_argument('--epoch', type=int, default=100)\n",
    "    args.add_argument('--batch_size', type=int, default=64)\n",
    "    args.add_argument('--num_classes', type=int, default=600)\n",
    "    args.add_argument('--input_shape', type=int, default=(224, 224, 3))\n",
    "    args.add_argument('--sbow_shape', type=int, default=(128,))\n",
    "    args.add_argument('--train', type=bool, default=False)\n",
    "    args.add_argument('--updateDB', type=bool, default=False)\n",
    "    args.add_argument('--DB_path', type=str, default=None)\n",
    "    args.add_argument('--model_path', type=str,\n",
    "                      default=\"./checkpoint/finish.hdf5\")\n",
    "    args.add_argument('--dataset_path', type=str, default=\"./data/images/\")\n",
    "    args.add_argument('--checkpoint_path', type=str, default=\"./checkpoint/\")\n",
    "    args.add_argument('--checkpoint_inteval', type=int, default=10)\n",
    "    args.add_argument('--reference_path', type=str, default=\"./\")\n",
    "\n",
    "    config = args.parse_args()\n",
    "\n",
    "    descriptor = Descriptor(config)\n",
    "\n",
    "    if config.train:\n",
    "\n",
    "        datagen = ImageDataGenerator(preprocessing_function=preprocess,\n",
    "                                     zoom_range=0.2, vertical_flip=True, horizontal_flip=True,\n",
    "                                     validation_split=0.1)\n",
    "\n",
    "        descriptor.train(config.dataset_path, datagen,\n",
    "                         checkpoint_path=config.checkpoint_path, checkpoint_inteval=config.checkpoint_inteval)\n",
    "\n",
    "    if config.updateDB:\n",
    "        descriptor.updateDB(config.model_path, config.DB_path, config.reference_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
