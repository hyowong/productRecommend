{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상세 페이지에서 크롤링 할 정보\n",
    "\n",
    "1. 스냅샷 \n",
    "<img src=\"https://image.musinsa.com/mfile_s01/_street_images/51263/800.street_5c94a6a2d06aa.jpg?20190322182728\" width=400>\n",
    "2. 관련 정보 \n",
    "<img src=\"https://www.dropbox.com/s/3edp92aczpiiwhl/Screenshot%202019-03-25%2013.00.28.png?raw=1\" width=400>\n",
    "3. 연결된 제품 \n",
    "<img src=\"https://www.dropbox.com/s/qhkjwieb5fzjue6/Screenshot%202019-03-25%2013.01.31.png?raw=1\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sub_crawler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_scrapper(page_number):\n",
    "    \"\"\"\n",
    "    scrapping a given page \n",
    "    \n",
    "    argument : 해당 페이지 번호\n",
    "    \n",
    "    return : 페이지 내의 사진 엘리먼트 리스트  \n",
    "    \"\"\"\n",
    "    url = \"https://www.musinsa.com/index.php?m=street&_mon=&p={}#listStart\".format(page_number)\n",
    "#     res = requests.session().post(url, data=login_info) \n",
    "    res = requests.get(url)\n",
    "    # raise error if response != 200 \n",
    "    res.raise_for_status() \n",
    "    return BeautifulSoup(res.content, 'html.parser').select(\"div .articleImg\")\n",
    "    \n",
    "def pic_link_parser(picture):\n",
    "    picture.select_one(\"a\")['href'].strip()\n",
    "    base_url = \"https://www.musinsa.com\"\n",
    "    return base_url + picture.select_one(\"a\")['href'].strip()\n",
    "\n",
    "\n",
    "def image_download(download_link, title, path, http=False):\n",
    "    if http :\n",
    "        pass\n",
    "    else :\n",
    "        download_link = \"http:\" + download_link\n",
    "    res = requests.get(download_link, stream=True)\n",
    "    \n",
    "    path += \"/{}\".format(title)\n",
    "    \n",
    "    with open(path, 'wb') as f:\n",
    "        \n",
    "        for chunk in res.iter_content(chunk_size=1024): \n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    return\n",
    "\n",
    "def get_styleInfo(soup):\n",
    "    \"\"\"\n",
    "    스냅샷의 정보 와 관련 태그 \n",
    "    \"\"\"\n",
    "    values = list(map(lambda x : x.text, soup.select_one(\"table\").select(\"td > span\")))\n",
    "    keys = list(map(lambda x : x.text, soup.select_one(\"table\").select(\"th > span\")))\n",
    "    related_tags = list(map(lambda x : x.text, soup.select_one(\"table\").select(\".listItem span\")))\n",
    "    \n",
    "    infoDict = {k : v for k, v in zip(keys, values)}\n",
    "    infoDict['tags'] = (\" \").join(list(map(lambda x : x.text, soup.select_one(\"table\").select(\".listItem span\"))))\n",
    "    \n",
    "    return infoDict\n",
    "\n",
    "def get_product_img(link):\n",
    "    res = requests.get(link)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        \n",
    "    img_url = soup.select_one(r\"div.product-img > img\")[\"src\"]\n",
    "\n",
    "    return img_url \n",
    "\n",
    "def item_spec(spec_url, id_=None, login_info=None, path=\".\", login_status=False):\n",
    "    \"\"\"\n",
    "    하나의 스냅샷 상세페이지에서 해당 사진과 연관된 패션아이템을 크롤링\n",
    "    \"\"\"\n",
    "    if not info:\n",
    "        print(\"you have no login infomation\")\n",
    "        raise ValueError\n",
    "        \n",
    "    sess = requests.session()\n",
    "    sess.post(\"https://www.musinsa.com/?mod=login\", data=login_info)\n",
    "    \n",
    "    res = sess.get(spec_url)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    \n",
    "    # 스냅샷 정보 \n",
    "    style_info = get_styleInfo(soup)\n",
    "    # 스냅샷 이미지 \n",
    "    snap_img = soup.select_one(\"div.snapImg > a > img\")['src']\n",
    "    snap_file_name = \"{}_snap.png\".format(id_)\n",
    "    style_info[\"Snapshot\"] = snap_file_name\n",
    "    image_download(snap_img, snap_file_name, path=path)\n",
    "    \n",
    "    # 스냅샷 아이템 확대 이미지 & 태그 \n",
    "    snap_part_imgs = soup.select_one(\"div > .styleItem-list\").select(\".itemImg\")\n",
    "    part_file_names = []\n",
    "    part_tags = []\n",
    "    \n",
    "    for i, part in enumerate(snap_part_imgs):\n",
    "        part_img = part.select_one(\"a > img\")[\"src\"]\n",
    "        part_file_name = \"{}_{}_part.png\".format(id_, i)\n",
    "        image_download(part_img, part_file_name, path=path)\n",
    "        part_file_names.append(part_file_name)\n",
    "        \n",
    "        part_spans = part.select(\"span\")\n",
    "        tags = []\n",
    "        for tag in part_spans:\n",
    "            tags.append(tag.get_text())\n",
    "            \n",
    "        part_tags.append(\"/\".join(tags))\n",
    "    \n",
    "    style_info[\"Part_snap\"] = (\" \").join(part_file_names)\n",
    "    style_info[\"Part_tag\"] = (\"&\").join(part_tags)\n",
    "    \n",
    "    # 추천된 아이템 이미지 \n",
    "    if login_status:\n",
    "        related_imgs_urls = get_item_img(spec_url)\n",
    "    else: \n",
    "        related_imgs_urls = get_item_img(spec_url, login_info=login_info)\n",
    "    \n",
    "    i = 0 \n",
    "    reco_imgs = []\n",
    "    for urls in related_imgs_urls:\n",
    "        block = []\n",
    "        for url in urls:\n",
    "            reco_img = get_product_img(url)\n",
    "            reco_file_name = \"{}_{}_reco.png\".format(id_, i)\n",
    "            image_download(reco_img, reco_file_name, path=path)\n",
    "            block.append(reco_file_name)\n",
    "            i += 1\n",
    "        reco_imgs.append(\" \".join(block))\n",
    "        \n",
    "    style_info[\"recommend_item\"] = (\"&\").join(reco_imgs)\n",
    "    \n",
    "    return style_info\n",
    "\n",
    "\n",
    "def main(start=1, end=10, id_, login_info, filename=\"train\"):\n",
    "    \"\"\"\n",
    "    arguments \n",
    "    \n",
    "    start : starting page to scraping \n",
    "    end : ending page to scraping\n",
    "    id_ : file_identifier \n",
    "    login_info : required dict object to login to the site \n",
    "    filename : train or test\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    dataset = []\n",
    "    login_status = False\n",
    "    \n",
    "    for i in range(start, end+1):\n",
    "        pages = page_scrapper(i)\n",
    "        spec_urls = list(map(pic_link_parser, pages))\n",
    "        \n",
    "        for url in spec_urls:\n",
    "            pre = time.time()\n",
    "            \n",
    "            style_info = item_spec(url, id_=id_, login_info=login_info, path=\"./data\", login_status=login_status)\n",
    "            \n",
    "            dataset.append(style_info)\n",
    "            \n",
    "            login_status = True\n",
    "            id_ += 1 \n",
    "            \n",
    "            print(\"{} is done, taken time : {}\".format(style_info[\"이름(나이)\"], time.time() - pre))\n",
    "\n",
    "            \n",
    "    df = pd.DataFrame(dataset)\n",
    "    \n",
    "    df.to_csv(\"./data/{0}/{0}.csv\".format(filename), index=False)\n",
    "    print(\"{} dataset complete\".format())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-289-967a75005358>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-289-967a75005358>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    main(start=1, end=10, id_=0, login_info, filename=\"train\"):\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "login_info = {\n",
    "    \"r\": \"home\",\n",
    "    \"a\": \"login\",\n",
    "    \"referer\": \"https://www.musinsa.com/index.php?\",\n",
    "    \"usessl\": \"0\",\n",
    "    \"id\": \"donotlose89\",\n",
    "    \"pw\": \"anfkzkal8992\",\n",
    "}\n",
    "\n",
    "main(start=1, end=10, id_=0, login_info, filename=\"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
